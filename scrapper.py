# -*- coding: utf-8 -*-
"""scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bg4WoJ_i2_FVdzIBuwbezYPXXP6jprvn
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
import joblib
import os
from datetime import datetime
from typing import List, Optional, Tuple

# Create output directory if it doesn't exist
os.makedirs("results", exist_ok=True)

# 1. Web Scraping Function
def scrape_karkidi_jobs(keyword: str = "data science", pages: int = 1) -> pd.DataFrame:
    """
    Scrape job listings from Karkidi.com for the given keyword and number of pages.
    """
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"üîç Scraping page: {page} for keyword: {keyword}")

        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # Raise HTTPError for bad requests
        except requests.RequestException as e:
            print(f"‚ö†Ô∏è Failed to retrieve page {page}: {e}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        if not job_blocks:
            print(f"‚ö†Ô∏è No job blocks found on page {page}")

        for job in job_blocks:
            try:
                # Extract title safely
                title_tag = job.find("h4")
                title = title_tag.get_text(strip=True) if title_tag else "N/A"

                # Extract company name by finding <a> tag with Employer-Profile in href
                company_tag = job.find("a", href=lambda x: x and "Employer-Profile" in x)
                company = company_tag.get_text(strip=True) if company_tag else "N/A"

                # Extract location (first <p> tag)
                location_tag = job.find("p")
                location = location_tag.get_text(strip=True) if location_tag else "N/A"

                # Extract experience, which has class 'emp-exp'
                exp_tag = job.find("p", class_="emp-exp")
                experience = exp_tag.get_text(strip=True) if exp_tag else "N/A"

                # Extract skills
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""

                # Extract summary
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"‚ö†Ô∏è Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be polite with server

    return pd.DataFrame(jobs_list)

# 2. Preprocessing
def preprocess_skills(df: pd.DataFrame) -> Tuple[pd.DataFrame, TfidfVectorizer]:
    """
    Preprocess skills by filling missing values, lowering case and vectorizing using TF-IDF.
    Returns normalized feature matrix and the vectorizer object.
    """
    df = df.copy()
    df["Skills"] = df["Skills"].fillna("").str.lower()

    vectorizer = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", stop_words="english")
    X = vectorizer.fit_transform(df["Skills"])

    X_norm = normalize(X)  # Normalize vectors to unit length

    return X_norm, vectorizer

# 3. Clustering
def cluster_skills(X, n_clusters: int = 5) -> KMeans:
    """
    Cluster TF-IDF skill vectors into n_clusters using KMeans.
    """
    model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    model.fit(X)
    return model

# 4. Save models
def save_model(model: KMeans, vectorizer: TfidfVectorizer,
               model_path: str = "job_cluster_model.pkl",
               vec_path: str = "tfidf_vectorizer.pkl") -> None:
    """
    Save the clustering model and vectorizer to disk.
    """
    joblib.dump(model, model_path)
    joblib.dump(vectorizer, vec_path)
    print(f"‚úÖ Saved model to {model_path} and vectorizer to {vec_path}")

# 5. Classify New Jobs
def classify_new_jobs(new_df: pd.DataFrame, model: KMeans, vectorizer: TfidfVectorizer) -> pd.DataFrame:
    """
    Classify new job listings into clusters based on skills.
    """
    new_df = new_df.copy()
    new_df["Skills"] = new_df["Skills"].fillna("").str.lower()
    X_new = vectorizer.transform(new_df["Skills"])
    X_new_norm = normalize(X_new)
    new_df["Cluster"] = model.predict(X_new_norm)
    return new_df

# 6. Notify User
def notify_user(new_df: pd.DataFrame, user_preferred_clusters: List[int]) -> pd.DataFrame:
    """
    Notify user about new jobs in preferred clusters.
    """
    matched_jobs = new_df[new_df["Cluster"].isin(user_preferred_clusters)]

    if not matched_jobs.empty:
        print("üö® New jobs found in your preferred categories:")
        print(matched_jobs[["Title", "Company", "Location", "Skills"]])
    else:
        print("‚úÖ No new jobs in your preferred categories today.")

    return matched_jobs

# 7. Daily Job Check
def run_daily_job_check(keyword: str,
                        user_clusters: List[int],
                        model: KMeans,
                        vectorizer: TfidfVectorizer,
                        pages: int = 1) -> pd.DataFrame:
    """
    Scrape new jobs, classify them, notify user and save results.
    """
    print(f"\nüîÑ Checking new jobs for keyword: '{keyword}'")
    new_jobs = scrape_karkidi_jobs(keyword=keyword, pages=pages)

    if new_jobs.empty:
        print("‚ö†Ô∏è No jobs scraped.")
        return pd.DataFrame()

    classified = classify_new_jobs(new_jobs, model, vectorizer)
    matched = notify_user(classified, user_clusters)

    # Save all scraped and matched jobs with timestamp
    now_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    all_jobs_path = f"results/{keyword.replace(' ', '_')}_all_jobs_{now_str}.csv"
    matched_jobs_path = f"results/{keyword.replace(' ', '_')}_matched_jobs_{now_str}.csv"

    classified.to_csv(all_jobs_path, index=False)
    matched.to_csv(matched_jobs_path, index=False)

    print(f"üìÅ Saved all jobs to: {all_jobs_path}")
    print(f"üìÅ Saved matched jobs to: {matched_jobs_path}")

    return matched

if __name__ == "__main__":
    # Step 1: Train model on initial scrape (run once)
    df_initial = scrape_karkidi_jobs(keyword="data science", pages=2)
    X, vec = preprocess_skills(df_initial)
    km_model = cluster_skills(X, n_clusters=5)
    save_model(km_model, vec)

    # Step 2: Load model and vectorizer (for later use)
    loaded_model = joblib.load("job_cluster_model.pkl")
    loaded_vec = joblib.load("tfidf_vectorizer.pkl")

    # Step 3: User prefers clusters 1 and 3, check new jobs (daily job check)
    matched_jobs = run_daily_job_check("data science", [1, 3], loaded_model, loaded_vec)

